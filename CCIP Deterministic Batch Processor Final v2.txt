Deterministic Batch Processor - 3 Part Prompt

BEGIN PART 1
python# -*- coding: utf-8 -*-
"""
CCIP Deterministic Batch Processor -- Part 1
Foundations, Inputs, Vectorised Scoring, Assets (SVGs + PNG conversion), Geometry, Utilities
British spelling, no em dashes. Deterministic behaviour only.
Optimized for Microsoft Forms input format.
"""

from __future__ import annotations
import os
import io
import re
import csv
import sys
import math
import json
import time
import hashlib
import warnings
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any

# Third party
import pandas as pd
import numpy as np

# Timezone handling for Pacific/Auckland
try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except Exception:
    ZoneInfo = None

# SVG to PNG conversion for Excel embedding convenience
try:
    import cairosvg
except Exception:
    cairosvg = None  # Part 1 defines functions, callers must ensure availability when converting

# ============================================================
# 0) Global configuration and constants
# ============================================================

LANGUAGE = "en-GB"
VARIANCE_TOLERANCE = 0  # identical inputs must produce identical outputs

DIMENSIONS = ["Directness", "Task vs Relational", "Conflict Orientation", "Cultural Adaptability", "Empathy Perspective"]
DIM_CODES = ["D", "TvR", "CO", "CA", "EP"]  # parallel to DIMENSIONS

# Strength and development tie priorities on unrounded floats
STRENGTH_TIE_PRIORITY = ["EP", "CA", "CO", "D", "TvR"]
DEVELOPMENT_TIE_PRIORITY = ["TvR", "D", "CO", "CA", "EP"]

# Icon thresholds by unrounded scores
STATUS_THRESHOLDS = [(0.00, 2.50, "âŒ"), (2.50, 3.00, "âœ”"), (3.00, 6.00, "âœ”âœ”")]
LEVEL_THRESHOLDS = [(0.00, 2.50, "ðŸ”§"), (2.50, 3.00, "ðŸŒ±"), (3.00, 6.00, "ðŸ›¡")]

# CSV headers in exact order (36 columns). Part 2 fills content.
CSV_HEADERS = [
    "Name",
    "Date",
    "Directness",
    "Directness Status Icon",
    "Directness Level Icon",
    "Task vs Relational",
    "Task vs Relational Status Icon",
    "Task vs Relational Level Icon",
    "Conflict Orientation",
    "Conflict Orientation Status Icon",
    "Conflict Orientation Level Icon",
    "Cultural Adaptability",
    "Cultural Adaptability Status Icon",
    "Cultural Adaptability Level Icon",
    "Empathy Perspective",
    "Empathy Perspective Status Icon",
    "Empathy Perspective Level Icon",
    "Strength 1",
    "Strength 1 Interpretation",
    "Strength 2",
    "Strength 2 Interpretation",
    "Development 1",
    "Development 1 Interpretation",
    "Development 2",
    "Development 2 Interpretation",
    "Development 3",
    "Development 3 Interpretation",
    "Communication Skills",
    "Conflict Resolution",
    "Cultural Awareness",
    "Reflective Question 1",
    "Reflective Question 2",
    "Reflective Question 3",
    "Reflective Question 4",
    "Summary",
    "Radar Chart SVG"
]

# Radar geometry constants (rendering occurs in Part 2, helpers here)
RADAR_VIEWBOX = "0 0 360 280"
RADAR_CX, RADAR_CY = 180.0, 140.0
RADAR_R = 110.0
# Angles clockwise from top, degrees
RADAR_ANGLES_DEG = [-90.0, -18.0, 54.0, 126.0, 198.0]
RADAR_LABELS = ["Directness", "Task/Relational", "Conflict", "Cultural", "Empathy"]

# Microsoft Forms exact Likert values
LIKERT_MS_FORMS = {
    "strongly disagree": 1.0,
    "disagree": 2.0,
    "neutral": 3.0,
    "agree": 4.0,
    "strongly agree": 5.0
}

# Deterministic PRNG avoidance guard
np.random.seed(0)

# ============================================================
# 1) Deterministic file discovery and preflight
# ============================================================

def discover_input_file(explicit_path: Optional[str] = None) -> str:
    """
    Simplified for Claude Project context:
    1) Check standard upload location: /mnt/user-data/uploads/
    2) Look for Excel or CSV files
    3) If multiple found, select most recent
    4) Fallback to explicit path if provided
    """
    upload_dir = "/mnt/user-data/uploads"
    
    if os.path.exists(upload_dir):
        files = []
        for fname in os.listdir(upload_dir):
            if fname.lower().endswith(('.xlsx', '.xls', '.csv')):
                fpath = os.path.join(upload_dir, fname)
                if os.path.isfile(fpath):
                    files.append(fpath)
        
        if files:
            # Sort by modification time, most recent first
            files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            return files[0]
    
    # Fallback to explicit path
    if explicit_path and os.path.isfile(explicit_path):
        return explicit_path
    
    raise FileNotFoundError("No survey file found. Please upload an Excel or CSV file.")

# ============================================================
# 2) Text normalisation and Likert mapping for Microsoft Forms
# ============================================================

def normalise_text(s: Any) -> str:
    """Simple normalisation for Microsoft Forms data - just trim and handle None."""
    if s is None or pd.isna(s):
        return ""
    return str(s).strip()

def to_key(s: str) -> str:
    """Lowercase for matching Microsoft Forms Likert values."""
    return normalise_text(s).lower()

@dataclass(frozen=True)
class LikertGlossary:
    """Simplified glossary for Microsoft Forms exact values."""
    map_exact: Dict[str, float]
    
    @classmethod
    def from_dataset(cls, df: pd.DataFrame, survey_slice: slice,
                     approved_aliases: Optional[Dict[str, float]] = None) -> "LikertGlossary":
        """
        Build mapping for Microsoft Forms Likert values.
        Only maps the 5 exact values we expect.
        """
        # For Microsoft Forms, we always use the exact 5 values
        return cls(map_exact=LIKERT_MS_FORMS.copy())
    
    def map_value(self, v: Any) -> Any:
        """Map Microsoft Forms Likert text to numeric value."""
        if v is None or pd.isna(v):
            return "NA"
        
        k = to_key(v)
        if not k:
            return "NA"
        
        # Direct lookup - Microsoft Forms values are consistent
        return self.map_exact.get(k, "NA")

# ============================================================
# 3) Vectorised numeric mapping, reverse coding, aggregation
# ============================================================

@dataclass
class ScoringInputs:
    item_to_dim: Dict[str, str]  # e.g., {"Q01": "D", ...}
    reverse_key: List[str]        # e.g., ["Q02","Q06","Q11","Q17"]
    # Part 2 will need band cut-offs. Not used here.

@dataclass
class ScoringOutputs:
    scores_unrounded: np.ndarray     # shape (n, 5)
    scores_rounded_str: np.ndarray   # shape (n, 5), "3.24"
    valid_counts: np.ndarray         # shape (n, 5), integer counts per dimension
    response_rate: np.ndarray        # shape (n,)
    data_quality: np.ndarray         # shape (n,), values in {"Complete","Partial Data","Insufficient Data"}
    numeric_matrix: np.ndarray       # shape (n, 25) numeric or "NA" for audit

def _build_index_arrays(item_to_dim: Dict[str, str]) -> Tuple[List[int], List[int], List[int], List[int], List[int]]:
    """
    Build index lists (0 based) for each dimension from item_to_dim mapping for Q01..Q25.
    """
    # Expected items in order: Q01..Q25 corresponding to dataframe columns I..AG
    # Convert to 0-based indices
    dim_to_items: Dict[str, List[int]] = {code: [] for code in DIM_CODES}
    for qnum in range(1, 26):
        qid = f"Q{qnum:02d}"
        dim_code = item_to_dim.get(qid)
        if dim_code not in DIM_CODES:
            raise ValueError(f"Bad item_to_dim: {qid} maps to {dim_code}, which is not one of {DIM_CODES}")
        dim_to_items[dim_code].append(qnum - 1)
    
    return (
        dim_to_items["D"],
        dim_to_items["TvR"],
        dim_to_items["CO"],
        dim_to_items["CA"],
        dim_to_items["EP"],
    )

def _reverse_mask(reverse_key: List[str]) -> np.ndarray:
    mask = np.zeros(25, dtype=bool)
    for q in reverse_key:
        m = re.match(r"Q(\d{2})$", q)
        if not m:
            raise ValueError(f"Bad reverse key item: {q}")
        idx = int(m.group(1)) - 1
        if not 0 <= idx < 25:
            raise ValueError(f"Bad reverse key index for {q}")
        mask[idx] = True
    return mask

def vectorise_numeric(df: pd.DataFrame, survey_slice: slice,
                      glossary: LikertGlossary, scoring_inputs: ScoringInputs) -> ScoringOutputs:
    """
    Vectorised path:
    1) Map text tokens to numeric using glossary
    2) Apply reverse coding where needed
    3) Aggregate unrounded means per dimension, require >=2 valid items
    4) Compute response rate and data quality
    5) Prepare rounded strings for display
    """
    # Extract survey values
    raw = df.iloc[:, survey_slice].astype(object).values  # n x 25
    n = raw.shape[0]
    
    # Map to numeric with vectorise
    vmap = np.vectorize(glossary.map_value, otypes=[object])
    numeric = vmap(raw)  # object array with floats or "NA"
    
    # Reverse code
    rmask = _reverse_mask(scoring_inputs.reverse_key)
    # Apply: x -> 6 - x on those indices where value is float
    for j in range(25):
        if rmask[j]:
            col = numeric[:, j]
            num_mask = np.array([isinstance(x, (int, float, np.floating)) for x in col], dtype=bool)
            numeric[num_mask, j] = 6.0 - np.asarray(col[num_mask], dtype=float)
    
    # Build dimension index arrays
    idx_D, idx_TvR, idx_CO, idx_CA, idx_EP = _build_index_arrays(scoring_inputs.item_to_dim)
    dim_indices = [idx_D, idx_TvR, idx_CO, idx_CA, idx_EP]
    
    # Compute per dimension means with >=2 valid responses
    scores_unrounded = np.zeros((n, 5), dtype=float)
    valid_counts = np.zeros((n, 5), dtype=int)
    
    for i in range(n):
        for d, jlist in enumerate(dim_indices):
            vals = []
            for j in jlist:
                v = numeric[i, j]
                if isinstance(v, (int, float, np.floating)):
                    vals.append(float(v))
            valid_counts[i, d] = len(vals)
            if len(vals) >= 2:
                scores_unrounded[i, d] = float(np.mean(vals))
            else:
                scores_unrounded[i, d] = 0.0  # mark as invalid for radar guards later
    
    # Response rate and quality
    na_counts = np.zeros(n, dtype=int)
    for i in range(n):
        na_counts[i] = sum(1 for v in numeric[i, :] if not isinstance(v, (int, float, np.floating)))
    
    answered = 25 - na_counts
    response_rate = answered / 25.0
    data_quality = np.where(
        response_rate >= 0.80, "Complete",
        np.where(response_rate >= 0.50, "Partial Data", "Insufficient Data")
    )
    
    # Rounded strings for display
    scores_rounded_str = np.vectorize(lambda x: f"{x:.2f}")(scores_unrounded)
    
    return ScoringOutputs(
        scores_unrounded=scores_unrounded,
        scores_rounded_str=scores_rounded_str,
        valid_counts=valid_counts,
        response_rate=response_rate,
        data_quality=data_quality,
        numeric_matrix=numeric
    )

# ============================================================
# 4) Selection logic, icons, and token helpers
# ============================================================

def _code_for_index(i: int) -> str:
    return DIM_CODES[i]

def select_strengths(unrounded: np.ndarray) -> List[int]:
    """
    Select top two dimensions by unrounded score.
    Tie break by STRENGTH_TIE_PRIORITY (EP > CA > CO > D > TvR).
    Deterministic and uses true floats to avoid rounding illusions.
    """
    out: List[int] = []
    # Build list of (idx, score, tie_rank)
    items = []
    for i, s in enumerate(unrounded):
        code = _code_for_index(i)
        tie_rank = STRENGTH_TIE_PRIORITY.index(code)
        items.append((i, float(s), tie_rank))
    # Sort by score desc, then tie_rank asc
    items.sort(key=lambda t: (-t[1], t[2], t[0]))
    out = [items[0][0], items[1][0]]
    return out

def select_development(unrounded: np.ndarray, strengths: List[int]) -> List[int]:
    """
    Strict complement of the three non-strength dimensions.
    Ascending by unrounded score. Tie break by DEVELOPMENT_TIE_PRIORITY (TvR > D > CO > CA > EP).
    """
    others = [i for i in range(5) if i not in strengths]
    items = []
    for i in others:
        code = _code_for_index(i)
        tie_rank = DEVELOPMENT_TIE_PRIORITY.index(code)
        items.append((i, float(unrounded[i]), tie_rank))
    # Sort by score asc, then tie_rank asc
    items.sort(key=lambda t: (t[1], t[2], t[0]))
    return [t[0] for t in items]

def status_icon(score_unrounded: float) -> str:
    for lo, hi, tok in STATUS_THRESHOLDS:
        if lo <= score_unrounded < hi:
            return tok
    return "?"

def level_icon(score_unrounded: float) -> str:
    for lo, hi, tok in LEVEL_THRESHOLDS:
        if lo <= score_unrounded < hi:
            return tok
    return "?"

# ============================================================
# 5) Icon assets: SVG sources, writers, and PNG conversion
# ============================================================

# Status Icons
SVG_STATUS_LOW_RED_CROSS = """<svg xmlns="http://www.w3.org/2000/svg" width="60" height="60" viewBox="0 0 720 720" version="1.1"><path d="M 330 1.051 C 263.786 7.369, 203.770 29.513, 150.737 67.195 C 76.566 119.895, 24.975 199.866, 7.002 289.995 C 1.714 316.516, 0.504 329.739, 0.553 360.500 C 0.607 395.528, 2.965 415.715, 10.597 446.500 C 34.723 543.818, 99.222 627.736, 186.500 675.360 C 256.179 713.381, 335.987 727.658, 413.500 715.969 C 528.969 698.556, 629.403 625.879, 681.472 522.055 C 699.238 486.631, 711.306 447.965, 716.444 410 C 719.637 386.409, 720.246 378.993, 720.657 358.667 C 720.945 344.409, 720.750 338.036, 720.039 338.476 C 719.381 338.882, 719 337.527, 719 334.783 C 719 306.690, 706.725 289.120, 684.250 285.045 C 667.414 281.993, 649.634 291.589, 641.740 307.990 C 638.218 315.306, 638.480 310.389, 639.023 359 C 639.283 382.264, 639.050 387.257, 637.116 399.893 C 627.720 461.274, 601.049 514.126, 557.565 557.538 C 534.329 580.735, 513.853 595.577, 485.466 609.798 C 461.398 621.855, 441.648 628.734, 416.082 633.963 C 362.532 644.915, 308.856 640.291, 257.500 620.301 C 243.398 614.812, 218.656 601.876, 205.500 593.115 C 187.656 581.231, 178.128 573.328, 161.472 556.596 C 113.431 508.335, 85.410 446.209, 80.919 378 C 76.374 308.958, 96.603 243.592, 140.029 187 C 150.327 173.580, 176.221 148.107, 190.136 137.708 C 230.196 107.771, 271.579 90.428, 321 82.865 C 333.004 81.028, 339.717 80.646, 360 80.646 C 380.283 80.646, 386.996 81.028, 399 82.865 C 415.236 85.350, 432.790 89.452, 447 94.084 C 462.814 99.238, 472.914 99.026, 484.338 93.302 C 498.554 86.177, 507.779 67.432, 504.905 51.507 C 502.400 37.626, 493.574 26.504, 480.873 21.224 C 463.675 14.074, 433.118 6.627, 405.851 2.940 C 388.332 0.572, 346.059 -0.481, 330 1.051 M 227.741 221.758 C 209.181 240.333, 206 243.951, 206 246.487 C 206 249.079, 213.088 256.551, 261.544 305.035 L 317.088 360.612 261.513 415.556 C 209.271 467.205, 205.940 470.707, 205.969 473.944 C 205.997 477.072, 207.865 479.266, 226.250 497.770 C 245.928 517.576, 249.971 521, 253.679 521 C 254.696 521, 279.251 497.221, 310.987 465.504 L 366.515 410.008 420.508 464.069 C 450.203 493.802, 475.715 518.775, 477.200 519.565 C 478.686 520.354, 480.686 521, 481.646 521 C 483.642 521, 524.416 480.894, 526.569 476.813 C 527.348 475.336, 527.705 473.311, 527.363 472.313 C 527.020 471.316, 501.937 445.637, 471.623 415.250 L 416.507 360 471.878 304.622 C 518.076 258.418, 527.250 248.790, 527.250 246.505 C 527.250 244.271, 523.220 239.734, 505.384 221.883 C 486.592 203.078, 483.094 200, 480.512 200 C 477.875 200, 470.742 206.763, 422.262 255.238 L 367.017 310.476 311.720 255.238 C 259.917 203.491, 256.204 200, 252.953 200 C 249.765 200, 247.713 201.771, 227.741 221.758 M 0.423 359.500 C 0.424 371.050, 0.570 375.638, 0.747 369.696 C 0.924 363.753, 0.923 354.303, 0.745 348.696 C 0.567 343.088, 0.422 347.950, 0.423 359.500" stroke="none" fill="#e74c3c" fill-rule="evenodd"/></svg>"""

SVG_STATUS_MID_GREEN_CHECK = """<svg xmlns="http://www.w3.org/2000/svg" width="60" height="60" viewBox="0 0 720 720" version="1.1"><path d="M 330 1.042 C 264.105 7.271, 203.703 29.561, 150.737 67.195 C 76.551 119.906, 24.979 199.847, 7.002 289.995 C 1.714 316.516, 0.504 329.739, 0.553 360.500 C 0.607 395.528, 2.965 415.715, 10.597 446.500 C 32.190 533.603, 85.580 609.531, 160.500 659.683 C 195.589 683.172, 235.678 700.359, 280 710.917 C 325.275 721.700, 377.826 722.791, 425.500 713.936 C 544.242 691.882, 643.895 612.015, 691.295 500.915 C 700.395 479.586, 708.628 452.783, 713.387 429 C 714.920 421.338, 717.122 405.966, 719.118 389 C 720.685 375.675, 721.379 337.648, 720.039 338.476 C 719.381 338.882, 719 337.527, 719 334.783 C 719 306.690, 706.725 289.120, 684.250 285.045 C 667.414 281.993, 649.634 291.589, 641.740 307.990 C 638.214 315.314, 638.448 310.943, 639.042 358.500 C 639.386 385.969, 638.466 396.482, 633.836 418 C 611.864 520.119, 535.643 601.181, 435 629.466 C 410.542 636.339, 387.224 639.410, 359.500 639.410 C 336.639 639.410, 327.445 638.552, 305.866 634.403 C 220.913 618.067, 146.453 560.754, 108.515 482.500 C 76.289 416.025, 71.314 339.296, 94.794 270.857 C 109.644 227.570, 130.420 194.353, 162.909 161.952 C 210.611 114.380, 270.025 86.915, 338 81.015 C 373.388 77.944, 412.790 82.771, 448.500 94.552 C 459.870 98.304, 468.094 98.774, 476.689 96.163 C 505.721 87.345, 515.015 51.592, 493.911 29.911 C 488.189 24.034, 482.625 21.189, 468 16.665 C 446.180 9.917, 428.460 6.013, 405.500 2.899 C 388.371 0.576, 345.968 -0.467, 330 1.042 M 543.419 200.817 C 518.767 216.245, 500.042 231.867, 468.942 262.953 C 430.144 301.734, 394.517 345.124, 359 396.851 L 348.500 412.144 346 409.216 C 344.625 407.606, 323.758 383.908, 299.630 356.554 L 255.760 306.819 251.133 311.160 C 244.409 317.467, 226.268 334.795, 212.858 347.720 L 201.216 358.940 222.858 385.856 C 234.761 400.660, 264.750 438.026, 289.500 468.892 L 334.500 525.012 354.500 524.466 C 376.930 523.855, 384 523.238, 384 521.894 C 384 520.210, 397.128 485.728, 403.178 471.523 C 439.984 385.102, 501.465 293.958, 571.202 222.429 L 583.946 209.357 568.541 202.179 C 560.068 198.230, 552.993 195.030, 552.818 195.067 C 552.643 195.104, 548.414 197.692, 543.419 200.817 M 0.423 359.500 C 0.424 371.050, 0.570 375.638, 0.747 369.696 C 0.924 363.753, 0.923 354.303, 0.745 348.696 C 0.567 343.088, 0.422 347.950, 0.423 359.500" stroke="none" fill="#27ae60" fill-rule="evenodd"/></svg>"""

SVG_STATUS_HIGH_GREEN_DOUBLE = """<svg xmlns="http://www.w3.org/2000/svg" width="60" height="60" viewBox="0 0 720 720" version="1.1"><path d="M 330 1.042 C 264.105 7.271, 203.703 29.561, 150.737 67.195 C 76.551 119.906, 24.979 199.847, 7.002 289.995 C 1.714 316.516, 0.504 329.739, 0.553 360.500 C 0.607 395.528, 2.965 415.715, 10.597 446.500 C 32.190 533.603, 85.580 609.531, 160.500 659.683 C 195.589 683.172, 235.678 700.359, 280 710.917 C 325.275 721.700, 377.826 722.791, 425.500 713.936 C 544.242 691.882, 643.895 612.015, 691.295 500.915 C 700.395 479.586, 708.628 452.783, 713.387 429 C 714.920 421.338, 717.122 405.966, 719.118 389 C 720.685 375.675, 721.379 337.648, 720.039 338.476 C 719.381 338.882, 719 337.527, 719 334.783 C 719 306.690, 706.725 289.120, 684.250 285.045 C 667.414 281.993, 649.634 291.589, 641.740 307.990 C 638.214 315.314, 638.448 310.943, 639.042 358.500 C 639.386 385.969, 638.466 396.482, 633.836 418 C 611.864 520.119, 535.643 601.181, 435 629.466 C 410.542 636.339, 387.224 639.410, 359.500 639.410 C 336.639 639.410, 327.445 638.552, 305.866 634.403 C 220.913 618.067, 146.453 560.754, 108.515 482.500 C 76.289 416.025, 71.314 339.296, 94.794 270.857 C 109.644 227.570, 130.420 194.353, 162.909 161.952 C 210.611 114.380, 270.025 86.915, 338 81.015 C 373.388 77.944, 412.790 82.771, 448.500 94.552 C 459.870 98.304, 468.094 98.774, 476.689 96.163 C 505.721 87.345, 515.015 51.592, 493.911 29.911 C 488.189 24.034, 482.625 21.189, 468 16.665 C 446.180 9.917, 428.460 6.013, 405.500 2.899 C 388.371 0.576, 345.968 -0.467, 330 1.042 M 391.386 234.391 C 371.539 244.955, 336.633 275.153, 309.246 305.450 C 285.815 331.372, 252.938 373.443, 237.500 397.260 C 234.750 401.503, 232.223 404.980, 231.885 404.987 C 231.547 404.994, 214.897 386.356, 194.885 363.569 C 174.873 340.781, 158.074 322.106, 157.553 322.069 C 157.032 322.031, 151.625 326.837, 145.538 332.750 C 139.452 338.663, 129.886 347.930, 124.280 353.344 L 114.089 363.188 120.344 370.844 C 123.785 375.055, 147.836 404.895, 173.793 437.156 L 220.986 495.812 233.243 495.373 C 239.984 495.132, 248.713 494.691, 252.640 494.392 L 259.780 493.850 263.016 484.675 C 291.096 405.064, 344.267 321.039, 407.882 255.749 C 414.179 249.286, 419.141 243.808, 418.909 243.576 C 418.677 243.343, 413.280 240.707, 406.918 237.717 L 395.349 232.281 391.386 234.391 M 620.903 236.789 C 577.819 263.550, 522.019 322.743, 474.118 392.500 C 469.586 399.100, 465.509 404.500, 465.058 404.500 C 464.607 404.500, 447.986 386.050, 428.122 363.500 C 408.258 340.950, 391.552 322.500, 390.996 322.500 C 389.910 322.500, 348 361.961, 348 362.983 C 348 363.321, 371.882 393.275, 401.072 429.548 L 454.144 495.500 469.322 495.128 C 477.670 494.923, 486.435 494.473, 488.800 494.128 L 493.099 493.500 499.924 475.500 C 528.513 400.091, 572.613 330.013, 632.260 265.206 C 640.002 256.795, 647.882 248.554, 649.773 246.895 C 651.663 245.235, 653.050 243.783, 652.855 243.668 C 652.202 243.285, 642.576 238.755, 635.403 235.455 L 628.306 232.190 620.903 236.789 M 0.423 359.500 C 0.424 371.050, 0.570 375.638, 0.747 369.696 C 0.924 363.753, 0.923 354.303, 0.745 348.696 C 0.567 343.088, 0.422 347.950, 0.423 359.500" stroke="none" fill="#27ae60" fill-rule="evenodd"/></svg>"""

# Level Icons
SVG_LEVEL_LOW_RED_TOOLS = """<svg xmlns="http://www.w3.org/2000/svg" width="60" height="60" viewBox="0 0 512.01 512.01" version="1.1"><g fill="#e74c3c"><g><path d="M477.765,373.495c-5.141-7.527-17.466-19.802-39.834-20.289c-17.809,4.379-36.815,11.154-54.572,21.35c-22.308,12.805-39.21,29.108-51.014,48.507c-0.171,0.547-0.282,0.915-0.282,0.915c-1.24,6.458-1.018,12.471,0.154,17.74c6.398,28.62,41.536,48.242,70.362,48.619c3.37,0.043,6.518-1.617,8.4-4.422c1.873-2.797,2.198-6.355,0.872-9.452l-17.415-40.629l38.457-19.228l16.945,39.552c1.257,2.934,3.849,5.081,6.963,5.782c3.114,0.701,6.372-0.128,8.767-2.241C489.287,438.904,494.949,398.548,477.765,373.495z"/></g></g><g fill="#e74c3c"><g><path d="M405.659,334.089c-44.932-30.955-86.75-64.066-124.472-98.306l78.111-78.111c1.839-1.839,1.839-4.824,0-6.663l-19.99-19.99c-1.839-1.839-4.824-1.839-6.663,0l-78.915,78.915c-30.647-29.938-58.079-60.525-81.464-91.147c-2.027-2.652-5.885-2.523-7.861,0.163c-4.362,5.936-9.169,11.59-14.481,16.902c-10.35,10.35-22.402,19.511-36.105,27.371c-8.793,5.055-18.005,9.358-27.354,13.027c-3.764,1.48-4.337,6.518-0.975,8.785c34.924,23.539,66.812,47.019,95.911,70.567L56.074,380.928L10.86,399.755c-1.745,1.163-3.045,2.874-3.712,4.858l-6.663,19.99c-1.129,3.387-0.248,7.117,2.275,9.64l53.314,53.314c2.523,2.523,6.261,3.404,9.64,2.275l19.99-6.663c1.984-0.659,3.695-1.967,4.858-3.712l18.809-45.223l129.031-129.031c31.511,29.655,58.72,59.516,81.764,89.804c1.959,2.575,5.714,2.566,7.775,0.068c2.942-3.593,6.056-7.074,9.392-10.401c10.358-10.35,22.41-19.519,36.113-27.38c9.965-5.722,20.469-10.512,31.092-14.49C408.336,341.385,408.994,336.39,405.659,334.089z"/></g></g><g fill="#e74c3c"><g><path d="M155.509,77.105c-6.39-28.62-41.536-48.242-70.362-48.619c-3.353-0.051-6.501,1.617-8.383,4.422c-1.873,2.797-2.198,6.355-0.881,9.452l17.415,40.63l-38.457,19.228l-16.945-39.56c-1.266-2.925-3.849-5.081-6.963-5.782c-3.114-0.701-6.372,0.128-8.767,2.241C-1.552,79.911-7.214,120.266,9.97,145.32c5.671,8.271,18.749,22.701,45.642,20.4c15.952-4.379,32.615-10.735,48.285-19.742c22.804-13.096,39.988-29.826,51.8-49.799c-0.094-0.137-0.205-0.282-0.308-0.419C155.56,95.213,156.681,82.374,155.509,77.105z"/></g></g><g fill="#e74c3c"><g><path d="M509.251,141.026l-19.99-19.99c-0.821-0.59-1.634-1.172-2.455-1.771l-25.233-12.617c-5.337-5.611-6.543-9.255-8.04-13.814c-1.659-5.021-3.729-11.265-10.914-18.45C383.761,15.528,316.171,5.443,257.194,46.731c-2.481,1.736-3.935,4.645-3.935,7.664v26.653c0,3.165,1.702,6.047,4.354,7.801c2.634,1.745,6.099,2.002,9.007,0.761c24.959-10.589,48.619-20.623,78.043-4.414l-4.285,12.856c-1.129,3.387-0.248,7.117,2.275,9.648l37.02,37.02c2.352,2.352,5.765,3.285,8.973,2.412c9.007-2.429,18.963-0.393,26.285,6.236l12.591,25.182c0.59,0.821,1.172,1.634,1.771,2.455l19.99,19.99c3.678,3.678,9.648,3.678,13.326,0l46.651-46.651C512.929,150.674,512.929,144.704,509.251,141.026z"/></g></g></svg>"""

SVG_LEVEL_MID_GREEN_SEEDLING = """<svg xmlns="http://www.w3.org/2000/svg" width="60" height="60" viewBox="0 0 444 445" version="1.1"><path d="M 324 21.667 C 322.625 21.883, 316.594 22.748, 310.598 23.587 C 289.624 26.524, 281.581 30.857, 276.631 41.888 C 274.133 47.454, 274.036 48.150, 275.330 51.246 C 277.547 56.554, 286.748 61.772, 295.500 62.687 C 300.109 63.168, 300.986 62.884, 306.717 59.044 C 310.137 56.753, 314.989 52.318, 317.500 49.189 C 322.022 43.556, 332 25.126, 332 22.409 C 332 21.013, 329.574 20.788, 324 21.667 M 269.431 71.749 C 267.020 82.667, 259.325 100.185, 251.673 112.175 C 249.804 115.104, 247.504 120.200, 246.562 123.500 C 244.698 130.028, 240.027 139.129, 236.637 142.839 C 234.889 144.751, 235.867 144.453, 242 141.204 C 246.125 139.019, 250.689 136.958, 252.142 136.624 C 254.806 136.011, 260.574 127.935, 270.800 110.500 C 275.330 102.776, 282.489 85.538, 283.517 79.878 C 284.139 76.452, 283.995 76.194, 280.837 75.094 C 279.002 74.455, 275.821 72.822, 273.769 71.465 L 270.039 68.997 269.431 71.749 M 185.500 77.064 C 174.972 78.105, 163.955 80.638, 138.500 87.868 C 126.950 91.148, 114.575 94.572, 111 95.477 C 107.425 96.381, 102.787 97.568, 100.694 98.114 L 96.889 99.107 117.194 119.346 C 128.362 130.477, 140.117 141.296, 143.314 143.389 C 155.704 151.494, 168.135 154.581, 182.011 152.999 C 205.999 150.263, 221.388 141.227, 229.173 125.305 C 232.231 119.049, 232.500 117.733, 232.500 109 C 232.500 100.337, 232.238 99.028, 229.532 94.141 C 223.938 84.041, 215.354 78.734, 201.882 77.044 C 197.301 76.470, 193.541 76.061, 193.526 76.136 C 193.512 76.211, 189.900 76.629, 185.500 77.064 M 255.423 151.820 C 245.829 155.137, 235.159 164.792, 231.604 173.372 C 228.034 181.991, 230.934 193.085, 238 197.838 C 241.070 199.903, 242.360 200.136, 248.500 199.729 C 261.055 198.897, 279.414 191.131, 298.219 178.698 C 315.590 167.212, 315.589 168.633, 298.230 160.562 C 275.352 149.925, 266.271 148.070, 255.423 151.820 M 178.500 207.100 C 127.070 209.854, 77.383 219.102, 66 228.037 L 63.500 230 66 231.990 C 72.151 236.884, 93.841 243.106, 118 246.906 C 174.958 255.865, 261.529 256.313, 319.250 247.947 C 351.185 243.319, 378.031 235.497, 379.730 230.326 C 380.287 228.632, 372.754 224.567, 363.500 221.568 C 347.799 216.479, 319.830 211.636, 291 209.013 L 278.500 207.875 267.500 211.600 C 258.013 214.813, 255.193 215.337, 247 215.412 C 238.976 215.486, 236.835 215.150, 233.219 213.250 C 230.865 212.012, 228.805 211, 228.642 211 C 228.479 211, 228.921 215.613, 229.625 221.250 C 230.329 226.887, 230.926 231.999, 230.952 232.609 C 230.996 233.632, 217.288 236.175, 215.899 235.401 C 215.569 235.217, 214.763 230.214, 214.107 224.283 C 213.452 218.353, 212.670 211.813, 212.370 209.750 L 211.824 206 202.662 206.148 C 197.623 206.230, 186.750 206.658, 178.500 207.100 M 70 252.826 C 70 256.923, 76.546 284.895, 77.848 286.360 C 78.756 287.383, 82.650 289.811, 86.500 291.755 C 115.920 306.610, 168.078 313.962, 235 312.688 C 281.652 311.800, 313.130 307.646, 340 298.833 C 350.755 295.305, 363.684 289.140, 366.154 286.361 C 367.714 284.607, 374.688 253.355, 373.733 252.400 C 373.501 252.168, 371.470 252.629, 369.219 253.423 C 327.218 268.241, 219.120 274.205, 142 265.958 C 116.405 263.221, 88.377 257.948, 76.124 253.565 C 71.159 251.789, 70 251.649, 70 252.826 M 113.435 357.377 L 118.500 397.877 121.500 401.029 C 126.110 405.871, 142.966 413.624, 156.072 416.930 C 194.921 426.731, 249.079 426.731, 287.928 416.930 C 301.257 413.568, 317.944 405.829, 322.633 400.836 L 325.765 397.500 330.750 357.500 C 333.491 335.500, 335.633 317.377, 335.510 317.226 C 335.386 317.075, 331.508 317.823, 326.892 318.888 C 314.524 321.741, 294.808 324.698, 275 326.672 C 253.244 328.839, 194.535 329.140, 174 327.189 C 149.974 324.906, 124.549 321.013, 110.935 317.532 L 108.370 316.876 113.435 357.377" stroke="none" fill="#27ae60" fill-rule="evenodd"/></svg>"""

SVG_LEVEL_HIGH_BLUE_SHIELD = """<svg xmlns="http://www.w3.org/2000/svg" width="60" height="60" viewBox="0 0 512 512"><path d="M479.07,111.36a16,16,0,0,0-13.15-14.74c-86.5-15.52-122.61-26.74-203.33-63.2a16,16,0,0,0-13.18,0C168.69,69.88,132.58,81.1,46.08,96.62a16,16,0,0,0-13.15,14.74c-3.85,61.11,4.36,118.05,24.43,169.24A349.47,349.47,0,0,0,129,393.11c53.47,56.73,110.24,81.37,121.07,85.73a16,16,0,0,0,12,0c10.83-4.36,67.6-29,121.07-85.73A349.47,349.47,0,0,0,454.64,280.6C474.71,229.41,482.92,172.47,479.07,111.36Zm-131,75.11-110.8,128A16,16,0,0,1,225.86,320h-.66a16,16,0,0,1-11.2-4.57l-49.2-48.2a16,16,0,1,1,22.4-22.86l37,36.29L323.9,165.53a16,16,0,0,1,24.2,20.94Z" fill="#3498db"/></svg>"""

def write_text(path: str, text: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def svg_to_png(svg_text: str, out_png_path: str, width: int = 40, height: int = 40) -> None:
    """
    Convert SVG text to PNG file with cairosvg. Deterministic size.
    """
    if cairosvg is None:
        raise RuntimeError("cairosvg not available. Install cairosvg to convert SVG to PNG.")
    os.makedirs(os.path.dirname(out_png_path), exist_ok=True)
    cairosvg.svg2png(bytestring=svg_text.encode("utf-8"),
                     write_to=out_png_path, output_width=width, output_height=height)

def materialise_icon_assets(base_dir: str = "./assets/icons") -> Dict[str, Dict[str, str]]:
    """
    Write SVG files and corresponding PNGs for all six icons.
    Returns a mapping of token -> {"svg": path, "png": path}
    """
    mapping: Dict[str, Dict[str, str]] = {}
    assets = [
        ("âŒ", "status_low_red_cross", SVG_STATUS_LOW_RED_CROSS),
        ("âœ”", "status_mid_green_check", SVG_STATUS_MID_GREEN_CHECK),
        ("âœ”âœ”", "status_high_green_double", SVG_STATUS_HIGH_GREEN_DOUBLE),
        ("ðŸ”§", "level_low_red_tools", SVG_LEVEL_LOW_RED_TOOLS),
        ("ðŸŒ±", "level_mid_green_seedling", SVG_LEVEL_MID_GREEN_SEEDLING),
        ("ðŸ›¡", "level_high_blue_shield", SVG_LEVEL_HIGH_BLUE_SHIELD),
    ]
    
    for token, stem, svg in assets:
        svg_path = os.path.join(base_dir, f"{stem}.svg")
        png_path = os.path.join(base_dir, f"{stem}.png")
        write_text(svg_path, svg)
        # Convert to 40x40 PNG for consistent Excel embedding
        if cairosvg is not None:
            svg_to_png(svg, png_path, width=40, height=40)
        mapping[token] = {"svg": svg_path, "png": png_path}
    
    return mapping

# ============================================================
# 6) Radar geometry primitives, minifier, and anchors
# ============================================================

def deg2rad(deg: float) -> float:
    return deg * math.pi / 180.0

def polar_to_cartesian(r: float, angle_deg: float, cx: float = RADAR_CX, cy: float = RADAR_CY) -> Tuple[float, float]:
    a = deg2rad(angle_deg)
    x = cx + r * math.cos(a)
    y = cy + r * math.sin(a)
    return (x, y)

def points_attr(points: List[Tuple[float, float]]) -> str:
    return " ".join(f"{x:.3f},{y:.3f}" for x, y in points)

def text_anchor_for_angle(angle_deg: float) -> str:
    # middle by default, start on right side, end on left side
    # Right side: angles in (-90, 90) exclusive
    if -90.0 < angle_deg < 90.0:
        return "start"
    # Left side: angles in (90, 270) exclusive
    if 90.0 < angle_deg < 270.0 or angle_deg < -90.0:
        return "end"
    return "middle"

def minify_svg(svg: str) -> str:
    """Single line, remove redundant whitespace without changing semantics."""
    s = re.sub(r">\s+<", "><", svg.strip())
    s = re.sub(r"\s{2,}", " ", s)
    s = s.replace("\n", "")
    return s

# ============================================================
# 7) Name and date precedence utilities with timezone
# ============================================================

ALLOWED_NAME_CHARS_RE = re.compile(r"^[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿ \.'\\-]{1,120}$")

def pick_name(colG: Any, colE: Any) -> str:
    """
    Primary source: column G 'Please type your name here'
    Fallback: column E 'Name'
    Preserve casing and diacritics. Validate allowed chars and length.
    """
    primary = normalise_text(colG)
    fallback = normalise_text(colE)
    chosen = primary if primary else fallback
    if not chosen:
        return "ERROR: missing input for name"
    if not ALLOWED_NAME_CHARS_RE.match(chosen):
        return "ERROR: validation failed for Name"
    return chosen

def pick_date(completion: Any, last_modified: Any, generated: Optional[pd.Timestamp] = None, 
              tz_name: str = "Pacific/Auckland") -> str:
    """
    Source priority: Completion time -> Last modified time -> GeneratedDate.
    Convert to Pacific/Auckland then format ISO YYYY-MM-DD.
    Handles Microsoft Forms format: M/D/YY H:M:S
    """
    # Parse with pandas
    candidate = None
    for v in [completion, last_modified, generated]:
        if v is None or pd.isna(v) or v == "":
            continue
        try:
            # Handle Microsoft Forms format
            if isinstance(v, str):
                # Try parsing with infer_datetime_format
                ts = pd.to_datetime(v, dayfirst=False, infer_datetime_format=True)
            else:
                ts = pd.to_datetime(v, utc=True)
            candidate = ts
            break
        except Exception:
            continue
    
    if candidate is None:
        return "ERROR: missing input for date"
    
    # Localize to timezone if not already timezone-aware
    if candidate.tz is None:
        try:
            candidate = candidate.tz_localize('UTC')
        except Exception:
            pass
    
    # Convert to target timezone
    if ZoneInfo is not None:
        try:
            local = candidate.tz_convert(ZoneInfo(tz_name))
        except Exception:
            local = candidate
    else:
        # Fall back to pandas timezone conversion
        try:
            local = candidate.tz_convert(tz_name)
        except Exception:
            local = candidate
    
    return local.strftime("%Y-%m-%d")

# ============================================================
# 8) Compression utilities for controlled-length prose
# ============================================================

STOPWORDS = set([
    "very", "really", "quite", "somewhat", "rather", "perhaps", "maybe",
    "that", "which", "who", "whom", "whose"
])

def compress_sentence(s: str, max_chars: int) -> str:
    """
    Deterministic compression pass for sentences:
    1) Trim spaces
    2) Remove repeated spaces
    3) Drop common hedges and redundant adverbs
    4) Collapse simple prepositional tails like 'in order to' -> 'to'
    5) Ensure terminal full stop
    """
    t = normalise_text(s)
    t = re.sub(r"\bin order to\b", "to", t, flags=re.IGNORECASE)
    words = t.split(" ")
    words = [w for w in words if to_key(w) not in STOPWORDS]
    t = " ".join(words)
    if not t.endswith("."):
        t += "."
    if len(t) <= max_chars:
        return t
    # If still too long, shave trailing phrases deterministically
    while len(t) > max_chars and "," in t:
        t = t.rsplit(",", 1)[0] + "."
    if len(t) > max_chars:
        # Final resort: drop last word groups
        parts = t[:-1].split(" ")
        while len(" ".join(parts)) + 1 > max_chars and len(parts) > 8:
            parts.pop()
        t = " ".join(parts) + "."
    return t

def compress_imperative(s: str, max_chars: int) -> str:
    """
    Compression for imperative lines. No terminal full stop.
    Remove hedges, compress 'in order to' to 'to', avoid commas where possible.
    """
    t = normalise_text(s)
    t = re.sub(r"\bin order to\b", "to", t, flags=re.IGNORECASE)
    t = re.sub(r",", "", t)
    words = t.split(" ")
    words = [w for w in words if to_key(w) not in STOPWORDS]
    t = " ".join(words)
    if len(t) <= max_chars:
        return t
    parts = t.split(" ")
    while len(" ".join(parts)) > max_chars and len(parts) > 4:
        parts.pop()
    return " ".join(parts)

# ============================================================
# 9) Lightweight caches and inspectors
# ============================================================

class PatternCache:
    """
    Quantised pattern cache for score tuples. Keys at 0.25 increments.
    """
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self._cache: Dict[Tuple[float, ...], Any] = {}
        self._hits = 0
        self._misses = 0
    
    @staticmethod
    def _quantise(scores: Tuple[float, ...], step: float = 0.25) -> Tuple[float, ...]:
        return tuple(round(s / step) * step for s in scores)
    
    def get_or_set(self, scores: Tuple[float, ...], generator) -> Any:
        key = self._quantise(scores)
        if key in self._cache:
            self._hits += 1
            return self._cache[key]
        self._misses += 1
        value = generator()
        if len(self._cache) < self.max_size:
            self._cache[key] = value
        return value
    
    def inspect(self) -> Dict[str, Any]:
        total = self._hits + self._misses
        hit_rate = float(self._hits) / total if total else 0.0
        return {
            "size": len(self._cache),
            "hit_rate": hit_rate,
            "hits": self._hits,
            "misses": self._misses,
        }

class ImageCache:
    """
    Simple in-memory cache for icon PNG bytes if needed.
    """
    def __init__(self):
        self._by_path: Dict[str, bytes] = {}
    
    def get_png_bytes(self, path: str) -> Optional[bytes]:
        if path in self._by_path:
            return self._by_path[path]
        if not os.path.isfile(path):
            return None
        with open(path, "rb") as f:
            data = f.read()
        self._by_path[path] = data
        return data

# ============================================================
# 10) CSV hygiene helpers
# ============================================================

def quote_csv_field(s: str) -> str:
    """
    Always wrap in double quotes. Escape embedded quotes by doubling.
    Literal newlines are allowed and preserved.
    """
    if s is None:
        s = ""
    t = str(s)
    t = t.replace('"', '""')
    return f'"{t}"'

def row_to_csv_line(row: List[str]) -> str:
    assert len(row) == 36, "Row must have exactly 36 columns"
    return ",".join(quote_csv_field(cell) for cell in row)

# ============================================================
# 11) Orchestration primitives for Part 1
# ============================================================

def load_input_dataframe(path: str) -> pd.DataFrame:
    """
    Load Excel or CSV into a DataFrame. Assumes survey columns are I..AG (0-based 8..32).
    Optimized for Microsoft Forms structure.
    """
    lower = path.lower()
    if lower.endswith(".csv"):
        df = pd.read_csv(path)
    elif lower.endswith(".xlsx") or lower.endswith(".xls"):
        df = pd.read_excel(path)
    else:
        raise ValueError(f"Unsupported file type: {path}")
    return df

def preflight_likert_glossary(df: pd.DataFrame, survey_slice: slice,
                              approved_aliases: Optional[Dict[str, float]] = None) -> LikertGlossary:
    return LikertGlossary.from_dataset(df, survey_slice=survey_slice, 
                                       approved_aliases=approved_aliases)

def compute_scoring(df: pd.DataFrame, scoring_inputs: ScoringInputs) -> ScoringOutputs:
    # Survey columns I..AG are 0-based slice(8, 33)
    survey_slice = slice(8, 33)
    glossary = preflight_likert_glossary(df, survey_slice)
    return vectorise_numeric(df, survey_slice, glossary, scoring_inputs)

def icon_asset_pipeline(base_dir: str = "./assets/icons") -> Dict[str, Dict[str, str]]:
    """
    Materialise six SVGs and PNGs, return token to file paths.
    """
    return materialise_icon_assets(base_dir=base_dir)
END PART 1

BEGIN PART 2

# -*- coding: utf-8 -*-
"""
CCIP Deterministic Batch Processor -- Part 2
Bands, narrative banks, recommendations, radar renderer, validators,
row assembly (36 columns), CSV writer, optional Excel builder with
image embedding.
British spelling, no em dashes. Deterministic behaviour only.

This Part 2 assumes Part 1 has already defined:
- CSV_HEADERS (Name, Date at columns 1-2)
- RADAR constants and helpers (RADAR_VIEWBOX, RADAR_CX, RADAR_CY, RADAR_R,
  RADAR_ANGLES_DEG, minify_svg, polar_to_cartesian, points_attr, text_anchor_for_angle)
- normalise_text, compress_sentence, compress_imperative
- status_icon, level_icon
- select_strengths, select_development
- pick_name, pick_date
- ScoringInputs, ScoringOutputs, compute_scoring
- row_to_csv_line, icon_asset_pipeline
"""

from __future__ import annotations

import os
import io
import re
import math
import hashlib
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any

# Optional Excel writer
try:
    from openpyxl import Workbook
    from openpyxl.utils import get_column_letter
    from openpyxl.drawing.image import Image as XLImage
except Exception:
    Workbook = None
    get_column_letter = None
    XLImage = None

# Optional SVG->PNG for radar image embedding in Excel
try:
    import cairosvg
except Exception:
    cairosvg = None

# ============================================================
# 12) Bands and cut-offs (deterministic defaults; overridable)
# ============================================================

# Default bands align with icon thresholds (Part 1)
BAND_CUTOFFS = {
    # intervals are [lo, hi) handled by band_of()
    "low":  (0.00, 2.50),
    "mid":  (2.50, 3.00),
    "high": (3.00, 6.00),
}

def band_of(score: float) -> str:
    if BAND_CUTOFFS["low"][0] <= score < BAND_CUTOFFS["low"][1]:
        return "low"
    if BAND_CUTOFFS["mid"][0] <= score < BAND_CUTOFFS["mid"][1]:
        return "mid"
    if BAND_CUTOFFS["high"][0] <= score < BAND_CUTOFFS["high"][1]:
        return "high"
    return "low"  # defensive default

# ============================================================
# 13) Narrative banks (expanded variety, deterministic selection)
# ============================================================

DIM_NAMES = ["Directness", "Task vs Relational", "Conflict Orientation", "Cultural Adaptability", "Empathy Perspective"]
DIM_CODES  = ["D", "TvR", "CO", "CA", "EP"]
CODE_TO_NAME = dict(zip(DIM_CODES, DIM_NAMES))

# Each dimension Ã— band has multiple options to reduce repetition at scale.
# Keep content concise; compression guards enforce final limits.
DIM_INTERP: Dict[str, Dict[str, List[str]]] = {
    "D": {
        "low": [
            "You tend to soften or delay difficult messages, which can create ambiguity about expectations.",
            "Indirect phrasing reduces clarity, risking misalignment on timelines and ownership.",
            "Caution with candour may limit timely feedback loops and slow decision cycles.",
            "Hesitancy to be direct can make trade offs and risks less visible to colleagues.",
            "Guarded communication sometimes obscures priorities and next steps."
        ],
        "mid": [
            "You strike a workable balance between clarity and care, adjusting directness to context.",
            "You can be clear when needed, while still maintaining consideration for others.",
            "Your messages are usually actionable, though tone shifts with audience and stakes.",
            "You moderate candour to preserve relationships, while aiming to keep plans explicit.",
            "You typically name issues, yet remain attentive to rapport and face saving."
        ],
        "high": [
            "You communicate with crisp clarity, surfacing issues early and enabling timely action.",
            "Direct, succinct messaging helps teams align on ownership, scope, and timelines.",
            "You make trade offs explicit, reducing rework and accelerating decisions.",
            "Clear expectations and fast feedback cycles are hallmarks of your style.",
            "You reduce uncertainty by naming risks and stating required next steps."
        ],
    },
    "TvR": {
        "low": [
            "Task focus may overshadow rapport building, reducing psychological safety in dialogue.",
            "You prioritise outputs over relationships, which can dampen collaboration over time.",
            "Tight delivery emphasis may limit the space for listening and context setting.",
            "Getting things done is central, yet some stakeholders may feel unheard.",
            "You push for closure quickly, which can miss relational signals that matter."
        ],
        "mid": [
            "You balance progress with people considerations, adapting as situations evolve.",
            "You can move work forward while tending to relationships when stakes are high.",
            "Your mix of task and rapport typically keeps momentum and buy in together.",
            "You combine delivery discipline with enough space for voices to be heard.",
            "You adjust pace and warmth to maintain traction across diverse teams."
        ],
        "high": [
            "You cultivate trust while driving execution, enabling sustained delivery.",
            "Relational sensitivity and task clarity coexist in your approach to work.",
            "You integrate empathy with accountability to keep outcomes and wellbeing aligned.",
            "You create conditions where candour and connection reinforce one another.",
            "You sequence relationship building to accelerate rather than delay delivery."
        ],
    },
    "CO": {
        "low": [
            "You avoid or defer tension, which can allow issues to persist beneath the surface.",
            "Preference for harmony may delay necessary conversations and limit learning.",
            "Conflict is often sidestepped, reducing opportunities to reset agreements.",
            "Unaddressed friction can compound into scope creep or role confusion.",
            "You sometimes trade short term ease for longer term clarity."
        ],
        "mid": [
            "You can enter or de escalate conflict as needed, choosing time and forum thoughtfully.",
            "You weigh costs of escalation against value of clarity, and adjust accordingly.",
            "You tend to engage constructively if stakes justify it.",
            "You de personalise disagreement, keeping focus on problems not people.",
            "You can frame tensions as joint work rather than personal fault."
        ],
        "high": [
            "You address tensions early and constructively, converting friction into alignment.",
            "You surface disagreement with care, protecting relationships while solving problems.",
            "You create structured spaces to work through differences and reach closure.",
            "You separate intent from impact, enabling repair and forward movement.",
            "You use conflict as signal for design or process improvement."
        ],
    },
    "CA": {
        "low": [
            "You rely on default norms, which may not travel well across cultures and contexts.",
            "Assumptions about hierarchy or disclosure can misfire with diverse teams.",
            "Limited adjustment to cultural cues can reduce inclusion and uptake.",
            "You sometimes apply one size fits all patterns across varied settings.",
            "You may under index on local context before choosing approach."
        ],
        "mid": [
            "You notice cultural cues and adapt some aspects of language or process.",
            "You adjust formality and sequencing to fit audience and setting.",
            "You sense when disclosure norms differ and calibrate accordingly.",
            "You vary decision pathways to respect local practices where possible.",
            "You check key assumptions with stakeholders before moving ahead."
        ],
        "high": [
            "You actively read context and tailor approach, increasing inclusion and impact.",
            "You flex hierarchy, time, and disclosure norms to enable participation.",
            "You translate across cultures, preserving intent while adapting expression.",
            "You proactively gather local insight before selecting methods.",
            "You support shared meaning despite diverse frames and constraints."
        ],
    },
    "EP": {
        "low": [
            "You may miss perspective taking opportunities, narrowing option discovery.",
            "Listening can be brief under pressure, reducing psychological safety.",
            "Assuming shared context can lead to mismatched expectations.",
            "You prioritise stating your view over drawing out others' reasoning.",
            "Signals of discomfort may be noticed but not acted upon."
        ],
        "mid": [
            "You invite views and reflect them back, enabling workable understanding.",
            "You can listen for needs beneath positions and respond proportionally.",
            "You notice non verbal cues and adjust pace and tone accordingly.",
            "You test your interpretations before acting on them.",
            "You combine inquiry with advocacy to co shape direction."
        ],
        "high": [
            "You use perspective taking to surface constraints and unlock cooperation.",
            "Active listening and curiosity broaden solution space and buy in.",
            "You notice and respond to subtle interpersonal signals early.",
            "You integrate empathy with clarity to anchor shared action.",
            "You help others feel seen while moving work forward."
        ],
    },
}

# Recommendations (28-30): multi line allowed by CSV hygiene rule.
RECOMMENDATIONS_BANK = {
    "communication": [
        "Use message maps: intent, three supports, call to action.\nTimebox feedback loops to 48 hours.",
        "Preface candour with care: context, concern, commitment.\nClose with explicit owner and date.",
        "Adopt 'BLUF' summaries in updates.\nReplace hedging with measurable next steps."
    ],
    "conflict": [
        "Frame tensions as joint design problems.\nUse issue statements: evidence, impact, request.",
        "Schedule short 'repair' conversations.\nName assumptions and agree verification steps.",
        "Create decision logs.\nSeparate people from problems, then agree exit criteria."
    ],
    "cultural": [
        "Run a five minute context check: hierarchy, time, disclosure, language, decision.",
        "Translate key terms and restate in recipients' idiom.\nInvite local examples before proposing fixes.",
        "Co design ceremony: who speaks first, when to challenge, how to close."
    ],
}

# Reflective questions (31-34): pick deterministically, single line each.
REFLECTIVE_QUESTIONS = [
    "What did I assume others already knew, and how could I test that early?",
    "Where might directness improve clarity without harming trust?",
    "What cultural cue did I notice and how did I adapt?",
    "What tension did I postpone, and what small step would surface it safely?",
    "Which stakeholder needs more listening before I ask for action?",
    "What would a concise BLUF for this update look like?",
    "Which term may be culture bound and needs translation?",
    "How can I separate intent from impact in the next difficult message?"
]

# Deterministic selector
def deterministic_choice(options: List[str], seed_text: str) -> str:
    if not options:
        return ""
    h = hashlib.sha1(seed_text.encode("utf-8")).hexdigest()
    idx = int(h[:8], 16) % len(options)
    return options[idx]

# ============================================================
# 14) Summary generator and char limits
# ============================================================

# Character limits (softly enforced by compression)
MAX_CHARS = {
    "strength_interp": 220,
    "development_interp": 220,
    "recommendation": 280,  # per cell; newlines allowed in 28-30
    "summary": 400
}

def make_summary(name: str, strengths_codes: List[str], dev_codes: List[str]) -> str:
    s1, s2 = [CODE_TO_NAME[c] for c in strengths_codes]
    d1, d2, d3 = [CODE_TO_NAME[c] for c in dev_codes]
    text = (
        f"{name} shows notable capability in {s1.lower()} and {s2.lower()}, "
        f"while focused development in {d1.lower()}, {d2.lower()}, and {d3.lower()} "
        f"would improve consistency across contexts."
    )
    return compress_sentence(text, MAX_CHARS["summary"])

# ============================================================
# 15) Radar renderer (SVG string in column 36)
# ============================================================

def _scale_score_to_radius(score: float, r_max: float) -> float:
    # Treat 1..5 as full range; 0 indicates invalid and returns 0 radius
    if score <= 0.0:
        return 0.0
    # clamp to [1,5]
    s = max(1.0, min(5.0, score))
    return (s - 1.0) / 4.0 * r_max

def render_radar_svg(scores: List[float], valid_flags: List[bool]) -> str:
    # points
    pts: List[Tuple[float, float]] = []
    for ang, sc, ok in zip(RADAR_ANGLES_DEG, scores, valid_flags):
        r = _scale_score_to_radius(sc if ok else 0.0, RADAR_R)
        pts.append(polar_to_cartesian(r, ang))
    polygon = points_attr(pts)

    # grid (3 rings at 25%, 50%, 75%)
    rings = [0.25, 0.50, 0.75]
    grid_circles = "".join(
        f'<circle cx="{RADAR_CX:.1f}" cy="{RADAR_CY:.1f}" r="{RADAR_R*g:.1f}" '
        f'stroke="#CBD5E1" stroke-width="1" fill="none"/>' for g in rings
    )
    # axes
    axes = "".join(
        f'<line x1="{RADAR_CX:.1f}" y1="{RADAR_CY:.1f}" '
        f'x2="{polar_to_cartesian(RADAR_R, ang)[0]:.1f}" y2="{polar_to_cartesian(RADAR_R, ang)[1]:.1f}" '
        f'stroke="#94A3B8" stroke-width="1"/>' for ang in RADAR_ANGLES_DEG
    )
    # labels
    labels = "".join(
        f'<text x="{polar_to_cartesian(RADAR_R+14, ang)[0]:.1f}" '
        f'y="{polar_to_cartesian(RADAR_R+14, ang)[1]:.1f}" '
        f'font-size="10" text-anchor="{text_anchor_for_angle(ang)}" '
        f'fill="#0F172A">{lbl}</text>'
        for ang, lbl in zip(RADAR_ANGLES_DEG, ["Directness","Task/Relational","Conflict","Cultural","Empathy"])
    )
    # filled polygon
    fill = (f'<polygon points="{polygon}" fill="#38BDF8" fill-opacity="0.25" '
            f'stroke="#0284C7" stroke-width="2"/>')

    svg = f'<svg xmlns="http://www.w3.org/2000/svg" viewBox="{RADAR_VIEWBOX}">{grid_circles}{axes}{fill}{labels}</svg>'
    return minify_svg(svg)

# ============================================================
# 16) Row assembly for a participant (36 columns)
# ============================================================

@dataclass
class AssembledRow:
    cells: List[str]  # length 36

def _interp_for(dim_code: str, score: float, seed: str, is_strength: bool) -> str:
    b = band_of(score)
    options = DIM_INTERP[dim_code][b]
    base = deterministic_choice(options, seed)
    limit = MAX_CHARS["strength_interp"] if is_strength else MAX_CHARS["development_interp"]
    return compress_sentence(base, limit)

def assemble_row_for_index(
    i: int,
    df,
    scoring: "ScoringOutputs",
    name_col_primary_idx: int = 6,  # G: typed name
    name_col_backup_idx: int = 4,   # E: Name
    completion_time_idx: int = 2,   # C
    last_modified_idx: int = 5,     # F
) -> AssembledRow:
    # Name & Date (columns 1-2)
    name_val = pick_name(df.iloc[i, name_col_primary_idx], df.iloc[i, name_col_backup_idx])
    date_val = pick_date(df.iloc[i, completion_time_idx], df.iloc[i, last_modified_idx])

    # Scores, icons
    u = scoring.scores_unrounded[i, :]  # 5 floats
    vc = scoring.valid_counts[i, :]     # 5 ints
    valid = [vc[d] >= 2 for d in range(5)]

    # Per dimension strings
    score_strs = [f"{u[d]:.2f}" if valid[d] else "" for d in range(5)]
    status = [status_icon(u[d]) if valid[d] else "?" for d in range(5)]
    level  = [level_icon(u[d])  if valid[d] else "?" for d in range(5)]

    # Strengths / Development selection on raw floats
    str_idx = select_strengths(u)
    dev_idx = select_development(u, str_idx)

    str_codes = [DIM_CODES[j] for j in str_idx]
    dev_codes = [DIM_CODES[j] for j in dev_idx]

    # Interpretations (deterministic choice)
    s1_interp = _interp_for(str_codes[0], u[str_idx[0]], f"{name_val}|{date_val}|{str_codes[0]}|strength|1", True)
    s2_interp = _interp_for(str_codes[1], u[str_idx[1]], f"{name_val}|{date_val}|{str_codes[1]}|strength|2", True)

    d1_interp = _interp_for(dev_codes[0], u[dev_idx[0]], f"{name_val}|{date_val}|{dev_codes[0]}|dev|1", False)
    d2_interp = _interp_for(dev_codes[1], u[dev_idx[1]], f"{name_val}|{date_val}|{dev_codes[1]}|dev|2", False)
    d3_interp = _interp_for(dev_codes[2], u[dev_idx[2]], f"{name_val}|{date_val}|{dev_codes[2]}|dev|3", False)

    # Recommendations (allow newlines)
    rec_comm = deterministic_choice(RECOMMENDATIONS_BANK["communication"], f"{name_val}|{date_val}|rec|comm")
    rec_conf = deterministic_choice(RECOMMENDATIONS_BANK["conflict"], f"{name_val}|{date_val}|rec|conf")
    rec_cult = deterministic_choice(RECOMMENDATIONS_BANK["cultural"], f"{name_val}|{date_val}|rec|cult")
    rec_comm = compress_sentence(rec_comm, MAX_CHARS["recommendation"])
    rec_conf = compress_sentence(rec_conf, MAX_CHARS["recommendation"])
    rec_cult = compress_sentence(rec_cult, MAX_CHARS["recommendation"])

    # Reflective questions (single line)
    rq1 = deterministic_choice(REFLECTIVE_QUESTIONS, f"{name_val}|{date_val}|rq|1")
    rq2 = deterministic_choice(REFLECTIVE_QUESTIONS, f"{name_val}|{date_val}|rq|2")
    rq3 = deterministic_choice(REFLECTIVE_QUESTIONS, f"{name_val}|{date_val}|rq|3")
    rq4 = deterministic_choice(REFLECTIVE_QUESTIONS, f"{name_val}|{date_val}|rq|4")

    # Summary
    summary = make_summary(name_val, str_codes, dev_codes)

    # Radar
    radar_svg = render_radar_svg(list(u), valid)

    # Assemble row in exact CSV_HEADERS order (36 columns)
    row = [
        name_val,                           # 1  Name
        date_val,                           # 2  Date
        score_strs[0], status[0], level[0], # 3-5 Directness
        score_strs[1], status[1], level[1], # 6-8 Task vs Relational
        score_strs[2], status[2], level[2], # 9-11 Conflict Orientation
        score_strs[3], status[3], level[3], # 12-14 Cultural Adaptability
        score_strs[4], status[4], level[4], # 15-17 Empathy Perspective
        CODE_TO_NAME[str_codes[0]],         # 18 Strength 1
        s1_interp,                          # 19 Strength 1 Interpretation
        CODE_TO_NAME[str_codes[1]],         # 20 Strength 2
        s2_interp,                          # 21 Strength 2 Interpretation
        CODE_TO_NAME[dev_codes[0]],         # 22 Development 1
        d1_interp,                          # 23 Development 1 Interpretation
        CODE_TO_NAME[dev_codes[1]],         # 24 Development 2
        d2_interp,                          # 25 Development 2 Interpretation
        CODE_TO_NAME[dev_codes[2]],         # 26 Development 3
        d3_interp,                          # 27 Development 3 Interpretation
        rec_comm,                           # 28 Communication Skills (multiline allowed)
        rec_conf,                           # 29 Conflict Resolution  (multiline allowed)
        rec_cult,                           # 30 Cultural Awareness   (multiline allowed)
        rq1, rq2, rq3, rq4,                 # 31-34 Reflective Questions
        summary,                            # 35 Summary
        radar_svg                           # 36 Radar Chart SVG
    ]
    return AssembledRow(row)

# ============================================================
# 17) Batch assembly, CSV writer, Excel builder
# ============================================================

def assemble_all_rows(df, scoring: "ScoringOutputs") -> List[List[str]]:
    rows: List[List[str]] = []
    for i in range(len(df)):
        assembled = assemble_row_for_index(i, df, scoring)
        # Data quality guard: keep the row, but if "Insufficient Data", interpretations remain;
        # callers can filter or flag externally if needed.
        rows.append(assembled.cells)
    return rows

def write_csv(out_path: str, rows: List[List[str]]) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True) if os.path.dirname(out_path) else None
    with open(out_path, "w", encoding="utf-8", newline="") as f:
        # header
        f.write(",".join(f'"{h}"' for h in CSV_HEADERS) + "\n")
        for row in rows:
            f.write(row_to_csv_line(row) + "\n")

# Columns containing icon tokens after Name/Date moved to front
ICON_COLS = [4,5, 7,8, 10,11, 13,14, 16,17]  # 1-based; Excel uses 1-based indexing
RADAR_COL = 36

def _cell_ref(row_idx_1based: int, col_idx_1based: int) -> str:
    # Only used for anchoring images via openpyxl; not needed for CSV
    col_letter = get_column_letter(col_idx_1based)
    return f"{col_letter}{row_idx_1based}"

def write_excel_with_images(out_xlsx_path: str, rows: List[List[str]], icon_assets: Dict[str, Dict[str, str]]) -> None:
    if Workbook is None or get_column_letter is None or XLImage is None:
        raise RuntimeError("openpyxl not available for Excel export.")
    wb = Workbook()
    ws = wb.active
    ws.title = "CCIP Reports"

    # Write header
    for j, h in enumerate(CSV_HEADERS, start=1):
        ws.cell(row=1, column=j, value=h)

    # Optional column sizing (light touch)
    preferred_widths = {
        1: 22, 2: 12, 3: 10, 6: 18, 9: 18, 12: 22, 15: 20,
        18: 16, 19: 42, 20: 16, 21: 42,
        22: 16, 23: 42, 24: 16, 25: 42, 26: 16, 27: 42,
        28: 46, 29: 46, 30: 46, 31: 36, 32: 36, 33: 36, 34: 36,
        35: 48, 36: 24
    }
    for col_idx, width in preferred_widths.items():
        ws.column_dimensions[get_column_letter(col_idx)].width = width

    # Write rows and insert icons / radar images
    base_img_dir = os.path.join("/mnt", "data", "ccip_assets")  # safe default in hosted environments
    os.makedirs(base_img_dir, exist_ok=True)

    for i, row in enumerate(rows, start=2):  # start from row 2 (after header)
        # values
        for j, val in enumerate(row, start=1):
            # For icon columns, we'll replace token with image if possible
            if j in ICON_COLS and isinstance(val, str) and val in icon_assets and os.path.isfile(icon_assets[val]["png"]):
                ws.cell(row=i, column=j, value="")  # keep cell empty; image will represent it
            else:
                ws.cell(row=i, column=j, value=val)

        # Insert icon images
        for j in ICON_COLS:
            token = row[j-1]
            if isinstance(token, str) and token in icon_assets:
                png_path = icon_assets[token]["png"]
                if os.path.isfile(png_path):
                    try:
                        img = XLImage(png_path)
                        img.width, img.height = 18, 18
                        ws.add_image(img, _cell_ref(i, j))
                    except Exception:
                        # Fallback: leave token text if image insert fails
                        ws.cell(row=i, column=j, value=token)
        # Insert radar image if cairosvg available
        radar_svg = row[RADAR_COL-1]
        if cairosvg is not None and isinstance(radar_svg, str) and radar_svg.strip().startswith("<svg"):
            png_out = os.path.join(base_img_dir, f"radar_{i-2:05d}.png")
            try:
                cairosvg.svg2png(bytestring=radar_svg.encode("utf-8"), write_to=png_out, output_width=380, output_height=296)
                img = XLImage(png_out)
                img.width, img.height = 190, 148
                ws.add_image(img, _cell_ref(i, RADAR_COL))
                # clear text to avoid huge XML
                ws.cell(row=i, column=RADAR_COL, value="")
            except Exception:
                # leave SVG text in cell if conversion fails
                pass

        # Slightly taller rows to fit icons
        ws.row_dimensions[i].height = 22

    # Header styling light touch (optional)
    ws.freeze_panes = "A2"
    wb.save(out_xlsx_path)

# ============================================================
# 18) End-to-end orchestration for Part 2
# ============================================================

def process_ccip_to_csv_and_excel(
    df,
    scoring_inputs: "ScoringInputs",
    out_csv_path: str,
    out_xlsx_path: Optional[str] = None,
    icon_base_dir: str = "./assets/icons"
) -> Dict[str, Any]:
    """
    Orchestrates scoring (Part 1), narrative assembly, and output.
    - df: DataFrame loaded from Microsoft Forms export
    - scoring_inputs: injected ITEM_TO_DIM and REVERSE_KEY
    - out_csv_path: CSV file to write
    - out_xlsx_path: optional Excel path with embedded icons and radar PNGs
    - icon_base_dir: where to materialise icon SVG/PNG assets
    """
    # Score
    scoring = compute_scoring(df, scoring_inputs)

    # Assemble rows
    rows = assemble_all_rows(df, scoring)

    # CSV
    write_csv(out_csv_path, rows)

    # Excel (optional)
    excel_written = False
    excel_error = None
    if out_xlsx_path:
        try:
            icon_assets = icon_asset_pipeline(icon_base_dir)
            write_excel_with_images(out_xlsx_path, rows, icon_assets)
            excel_written = True
        except Exception as e:
            excel_error = str(e)

    return {
        "n_rows": len(rows),
        "csv_path": out_csv_path,
        "xlsx_path": out_xlsx_path if excel_written else None,
        "excel_error": excel_error,
    }


END PART 2

BEGIN PART 3

# -*- coding: utf-8 -*-
"""
CCIP Deterministic Batch Processor -- Part 3
Auto-configuration from survey headers (Microsoft Forms), reverse-key detection,
end-to-end wrapper, determinism checks, and CLI-style entrypoint.

Assumes Parts 1 and 2 have been pasted above in the same prompt/session.
British spelling, no em dashes. Deterministic behaviour only.
"""

from __future__ import annotations

import os
import re
import io
import csv
import json
import time
import math
import hashlib
from typing import Dict, List, Tuple, Optional, Any

# Pandas/Numpy are expected from Part 1, but import defensively:
try:
    import pandas as pd
    import numpy as np
except Exception as _e:
    raise RuntimeError("pandas and numpy are required for CCIP processing.") from _e


# ============================================================
# 19) Auto-config from Microsoft Forms headers
#      -> Derive item_to_dim (Q01..Q25 -> D/TvR/CO/CA/EP)
#      -> Derive reverse_key list (e.g., ['Q02','Q06',...])
# ============================================================

def _norm_header(h: Any) -> str:
    """Lightweight normalisation for header text (lowercase, compress spaces)."""
    if h is None:
        return ""
    t = str(h).lower().strip()
    t = re.sub(r"\s+", " ", t)
    return t

# Keyword dictionaries (weights) per dimension for header scoring
# Tailored to the dataset you shared; extend if your Forms wording grows.
_DIM_KEYWORDS: Dict[str, Dict[str, int]] = {
    "D": {  # Directness
        "direct": 3, "clear": 2, "transparent": 2, "candour": 2, "honest": 2,
        "feedback": 1, "explicit": 2, "disclose": 1, "challenge": 1
    },
    "TvR": {  # Task vs Relational
        "task": 2, "deadline": 2, "deliver": 1, "accountable": 1,
        "relationship": 3, "harmony": 2, "rapport": 2, "balance": 2, "trust": 1
    },
    "CO": {  # Conflict Orientation
        "conflict": 4, "confront": 3, "tension": 2, "disagree": 2,
        "negative feedback": 2, "repair": 2, "escalate": 1, "avoid confrontation": 4,
        "compromise": 1, "address": 1
    },
    "CA": {  # Cultural Adaptability
        "culture": 4, "cultural": 4, "hierarchy": 3, "seniority": 2,
        "disclosure": 2, "local": 2, "background": 1, "translate": 1, "adapt": 2
    },
    "EP": {  # Empathy & Perspective-Taking
        "empathy": 4, "perspective": 3, "help me understand": 4, "listen": 3,
        "listening": 3, "non-verbal": 3, "tone": 2, "body language": 2, "expressions": 2,
        "see": 1, "feel": 1, "understand": 2, "curiosity": 2
    },
}

# Negative-polarity indicators for reverse coding (agreement means lower latent trait)
_REVERSE_HINTS = [
    "avoid", "soften", "defer", "too direct", "too indirect", "uncomfortable",
    "hesitant", "compromise", "sidestep", "delay", "postpone"
]

def _header_scores_for_dims(header_norm: str) -> Dict[str, int]:
    """Compute a simple additive score per dimension using keyword weights."""
    scores = {k: 0 for k in _DIM_KEYWORDS.keys()}
    for dim, kw_map in _DIM_KEYWORDS.items():
        s = 0
        for phrase, w in kw_map.items():
            if phrase in header_norm:
                s += w
        scores[dim] = s
    return scores

def _initial_assignments(headers: List[str]) -> Tuple[Dict[str, str], Dict[str, List[int]]]:
    """
    First pass: assign each question to the dimension with the highest header score.
    Returns (item_to_dim, dim_to_indices)
    """
    item_to_dim: Dict[str, str] = {}
    dim_to_indices: Dict[str, List[int]] = {k: [] for k in _DIM_KEYWORDS.keys()}

    for k in range(25):
        qid = f"Q{k+1:02d}"
        h = _norm_header(headers[k])
        scores = _header_scores_for_dims(h)
        # choose best dim, tie-break by fixed order D, TvR, CO, CA, EP for determinism
        order = ["D", "TvR", "CO", "CA", "EP"]
        best_dim = max(order, key=lambda d: (scores[d], 100 - order.index(d)))
        item_to_dim[qid] = best_dim
        dim_to_indices[best_dim].append(k)
    return item_to_dim, dim_to_indices

def _rebalance_to_five_each(item_to_dim: Dict[str, str],
                            headers: List[str]) -> Dict[str, str]:
    """
    Ensure exactly five items per dimension. Move the weakest-scoring items from
    overfull dims to underfull dims, picking next-best dimension by header scores.
    """
    # Build helper structures
    scores_per_q: Dict[str, Dict[str, int]] = {}
    for k in range(25):
        qid = f"Q{k+1:02d}"
        h = _norm_header(headers[k])
        scores_per_q[qid] = _header_scores_for_dims(h)

    target = 5
    dims = ["D", "TvR", "CO", "CA", "EP"]

    def counts() -> Dict[str, int]:
        c = {d: 0 for d in dims}
        for qid, d in item_to_dim.items():
            c[d] += 1
        return c

    # Iteratively adjust until each has exactly 5
    max_iters = 100
    for _ in range(max_iters):
        c = counts()
        over = [d for d in dims if c[d] > target]
        under = [d for d in dims if c[d] < target]
        if not over and not under:
            break

        if over and under:
            # Move one weakest from the most-overfull to the most-underfull
            d_over = max(over, key=lambda d: c[d])  # most extra
            d_under = min(under, key=lambda d: c[d])  # most deficit

            # Candidates in d_over sorted by (score diff best-other, ascending)
            cands = [qid for qid, dd in item_to_dim.items() if dd == d_over]
            # For each candidate, compute its next-best dim (excluding current)
            ranked: List[Tuple[str, int, str]] = []
            for qid in cands:
                q_scores = scores_per_q[qid]
                # Next best excludes current dim, prefer d_under if tied
                others = [d for d in dims if d != d_over]
                best_other = max(others, key=lambda d: (q_scores[d], 1000 if d == d_under else 0))
                gap = q_scores[d_over] - q_scores[best_other]
                ranked.append((qid, gap, best_other))
            ranked.sort(key=lambda t: (t[1], qid))  # smallest gap first (least harm)

            # Move the best candidate towards the underfull, but only if its best_other is d_under,
            # otherwise move still but prefer that direction for stability.
            qid_move, _, best_other = ranked[0]
            new_dim = d_under if best_other == d_under else best_other
            item_to_dim[qid_move] = new_dim
            continue

        # Safety valve: if over but no under (or vice versa), redistribute round-robin
        if over and not under:
            # move arbitrary item to the dim with the smallest count (should not happen often)
            d_src = over[0]
            qid_move = next(q for q, dd in item_to_dim.items() if dd == d_src)
            d_dst = min(dims, key=lambda d: sum(1 for _q, _d in item_to_dim.items() if _d == d))
            if d_dst != d_src:
                item_to_dim[qid_move] = d_dst
        elif under and not over:
            # steal from the largest
            d_dst = under[0]
            d_src = max(dims, key=lambda d: sum(1 for _q, _d in item_to_dim.items() if _d == d))
            qid_move = next(q for q, dd in item_to_dim.items() if dd == d_src)
            item_to_dim[qid_move] = d_dst

    # Final assert
    final_counts = {d: 0 for d in dims}
    for _q, d in item_to_dim.items():
        final_counts[d] += 1
    if any(final_counts[d] != 5 for d in dims):
        # As a last resort, force balance deterministically
        # Fill dims under 5 by reassigning the earliest Q* not yet adjusted
        needed = [(d, 5 - final_counts[d]) for d in dims if final_counts[d] < 5]
        excess = [(d, final_counts[d] - 5) for d in dims if final_counts[d] > 5]
        # Expand lists
        need_list = []
        for d, k in needed:
            need_list.extend([d] * k)
        excess_qs = []
        for d, k in excess:
            qs = [q for q, dd in item_to_dim.items() if dd == d]
            excess_qs.extend(qs[:k])
        # Reassign in order
        for qid, d_target in zip(excess_qs, need_list):
            item_to_dim[qid] = d_target

    return item_to_dim

def _derive_reverse_key(headers: List[str], item_to_dim: Dict[str, str]) -> List[str]:
    """
    Heuristic reverse-key detection: if header suggests 'avoid/soften/defer' etc.,
    mark as reverse for the mapped dimension. Deterministic and conservative.
    """
    rev: List[str] = []
    for k in range(25):
        qid = f"Q{k+1:02d}"
        h = _norm_header(headers[k])
        if any(tok in h for tok in _REVERSE_HINTS):
            rev.append(qid)
        # Special cases that imply positive polarity, ensure not reversed
        if "comfortable giving negative feedback" in h:
            if qid in rev:
                rev.remove(qid)
    # Deduplicate, keep order
    seen = set()
    out = []
    for q in rev:
        if q not in seen:
            out.append(q)
            seen.add(q)
    return out

def derive_scoring_inputs_from_df(df: pd.DataFrame,
                                  survey_slice: slice = slice(8, 33)) -> "ScoringInputs":
    """
    Build ScoringInputs from the Microsoft Forms headers in df.
    Guarantees balanced mapping (5 items per dimension).
    """
    headers = list(df.columns[survey_slice])
    if len(headers) != 25:
        raise ValueError(f"Expected 25 survey columns, found {len(headers)} in slice {survey_slice}.")

    initial_map, _ = _initial_assignments(headers)
    item_to_dim = _rebalance_to_five_each(initial_map, headers)
    reverse_key = _derive_reverse_key(headers, item_to_dim)

    return ScoringInputs(item_to_dim=item_to_dim, reverse_key=reverse_key)


# ============================================================
# 20) End-to-end wrapper helpers and public API
# ============================================================

def _timestamp() -> str:
    return time.strftime("%Y%m%d_%H%M%S", time.gmtime())

def ccip_output_paths(out_dir: str = "./ccip_out",
                      stem: str = "ccip_reports") -> Tuple[str, str]:
    """
    Return (csv_path, xlsx_path) using a UTC timestamp in filenames.
    """
    os.makedirs(out_dir, exist_ok=True)
    ts = _timestamp()
    csv_path = os.path.join(out_dir, f"{stem}_{ts}.csv")
    xlsx_path = os.path.join(out_dir, f"{stem}_{ts}.xlsx")
    return csv_path, xlsx_path

def run_ccip_pipeline(input_path: Optional[str] = None,
                      out_dir: str = "./ccip_out",
                      make_excel: bool = True,
                      icon_base_dir: str = "./assets/icons",
                      scoring_inputs: Optional["ScoringInputs"] = None) -> Dict[str, Any]:
    """
    Convenience wrapper:
      - Locate or accept input_path
      - Load DataFrame
      - Derive scoring inputs from headers if not provided
      - Process to CSV (and XLSX if enabled)
    Returns a dict summary with file paths and counts.
    """
    # 1) Locate file
    if not input_path:
        input_path = discover_input_file()

    # 2) Load DF
    df = load_input_dataframe(input_path)

    # 3) Scoring inputs
    if scoring_inputs is None:
        scoring_inputs = derive_scoring_inputs_from_df(df)

    # 4) Output paths
    csv_path, xlsx_path = ccip_output_paths(out_dir)
    if not make_excel:
        xlsx_path = None

    # 5) Run orchestrator from Part 2
    result = process_ccip_to_csv_and_excel(
        df=df,
        scoring_inputs=scoring_inputs,
        out_csv_path=csv_path,
        out_xlsx_path=xlsx_path,
        icon_base_dir=icon_base_dir
    )
    result.update({
        "input_path": input_path,
        "item_to_dim": scoring_inputs.item_to_dim,
        "reverse_key": scoring_inputs.reverse_key,
    })
    return result


# ============================================================
# 21) Determinism checks and light validations
# ============================================================

def hash_rows(rows: List[List[str]]) -> str:
    """
    Deterministic hash of assembled rows content. Use to verify reproducibility.
    """
    h = hashlib.sha256()
    for r in rows:
        for c in r:
            # Normalize newlines to \n for stable hashing
            t = "" if c is None else str(c).replace("\r\n", "\n").replace("\r", "\n")
            h.update(t.encode("utf-8"))
            h.update(b"\x00")  # cell separator
        h.update(b"\xFF")      # row separator
    return h.hexdigest()

def self_check_determinism(df: pd.DataFrame,
                           scoring_inputs: Optional["ScoringInputs"] = None) -> Dict[str, Any]:
    """
    Assemble twice in memory and compare hashes. Raises if mismatch.
    """
    if scoring_inputs is None:
        scoring_inputs = derive_scoring_inputs_from_df(df)
    scoring = compute_scoring(df, scoring_inputs)
    rows_a = assemble_all_rows(df, scoring)
    rows_b = assemble_all_rows(df, scoring)
    ha, hb = hash_rows(rows_a), hash_rows(rows_b)
    if ha != hb:
        raise AssertionError("Determinism check failed: content hashes differ.")
    return {
        "rows": len(rows_a),
        "hash": ha,
        "valid_counts_example": scoring.valid_counts[0, :].tolist() if len(df) else [],
        "response_rate_example": float(scoring.response_rate[0]) if len(df) else None,
    }

def sanity_check_headers(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Quick header presence check for the metadata columns we expect from Microsoft Forms.
    """
    expected_meta = {
        "ID", "Start time", "Completion time", "Email", "Name",
        "Last modified time",
        "Please type your name here so a personal report can be created - your results will not be shared with anyone",
        "Please type your email address here so a personal report can be created - your results will not be shared with anyone",
    }
    cols = set(map(str, df.columns[:8]))
    missing = [c for c in expected_meta if c not in cols]
    return {"missing_meta_columns": missing, "ok": len(missing) == 0}


# ============================================================
# 22) CLI-style entrypoint (optional) -- safe to call from LLM tool
# ============================================================

def main():
    """
    Minimal CLI-like entrypoint:
      - Discovers the newest upload (Excel/CSV) if no explicit path set via CCIP_INPUT
      - Writes CSV, attempts XLSX with embedded icons + radar PNGs (if cairosvg/openpyxl available)
      - Prints a JSON summary to stdout
    Environment variables (optional):
      CCIP_INPUT: explicit path to input file
      CCIP_OUTDIR: output directory (default ./ccip_out)
      CCIP_NO_XLSX: set to '1' to skip Excel creation
    """
    input_path = os.environ.get("CCIP_INPUT", None)
    out_dir = os.environ.get("CCIP_OUTDIR", "./ccip_out")
    make_excel = os.environ.get("CCIP_NO_XLSX", "0") != "1"

    try:
        result = run_ccip_pipeline(
            input_path=input_path,
            out_dir=out_dir,
            make_excel=make_excel,
            icon_base_dir="./assets/icons",
            scoring_inputs=None  # auto-derive from headers
        )
        # Determinism smoke test on a subset if feasible
        try:
            df = load_input_dataframe(result["input_path"])
            _ = self_check_determinism(df)
        except Exception:
            # Non-fatal: we still return the main result
            pass

        print(json.dumps(result, indent=2))
    except Exception as e:
        print(json.dumps({"error": str(e)}))
        raise

if __name__ == "__main__":
    main()


END PART 3</document_content></document>
</documents>
This is 